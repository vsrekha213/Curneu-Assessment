# -*- coding: utf-8 -*-
"""malariya xgboost.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-PYMx5hdSxhnX5_MSJmrd1O7t6vO5oaJ
"""

import pandas as pd
import numpy as np

from google.colab import files
uploaded = files.upload()

cases= pd.read_csv('estimated_numbers.csv')
cases.head()

incidence = pd.read_csv('incidence_per_1000_pop_at_risk.csv')
incidence.head()

reports = pd.read_csv('reported_numbers.csv')
reports.head()

len(incidence['Year'].unique())

cases['Year'].unique()

len(reports['Year'].unique())

reports.shape

reports.isnull().sum()

reports.dtypes

#reports = reports.dropna()

'''from sklearn.preprocessing import MinMaxScaler, LabelEncoder
cols = ['Country', 'WHO Region']
for col in cols:
    le = LabelEncoder()
    letest = LabelEncoder()
    reports[col] = le.fit_transform(reports[col])
""""'''

reports = pd.get_dummies(reports,drop_first=True)

reports.isnull().sum()

reports = reports.fillna(0)

from sklearn.impute import KNNImputer
imputer = KNNImputer(n_neighbors=5, weights='uniform', metric='nan_euclidean')

#reports['No. of cases'].fillna((reports['No. of cases'].mean()), inplace=True)

#reports['No. of deaths'].fillna((reports['No. of deaths'].mean()), inplace=True)

imputer.fit(reports)

report_fill = imputer.fit_transform(reports)

report_fill = pd.DataFrame(report_fill)

report_fill

report_fill.isnull().sum()

reports.head()

reports.shape

report_fill.shape

reports

from sklearn.preprocessing import StandardScaler
sc_X = StandardScaler()
sc_X = sc_X.fit_transform(reports)
#Convert to table format - StandardScaler 
sc_X = pd.DataFrame(data=sc_X)
sc_X

y = sc_X[2]
y = pd.DataFrame(y)
y

X = sc_X.drop(columns = 2, axis=1).reset_index(drop=True)
X

from sklearn.metrics import r2_score

y = reports['No. of deaths']
X = reports.drop(columns = ['No. of deaths'], axis=1).reset_index(drop=True)

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)

from sklearn.ensemble import RandomForestRegressor

rf = RandomForestRegressor(random_state = 42)

rf.fit(X_train,y_train)

yp = rf.predict(X_test)

from sklearn.metrics import r2_score

r2_score(y_test, yp)

from sklearn.model_selection import RandomizedSearchCV

from sklearn.model_selection import RandomizedSearchCV
rs = RandomizedSearchCV(RandomForestRegressor(random_state = 42), {
        'n_estimators' : [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)],
        'max_features' : ['auto', 'sqrt'],
        'max_depth' : [int(x) for x in np.linspace(10, 110, num = 11)],
        'min_samples_split' : [2, 5, 10],
        'min_samples_leaf' : [1, 2, 4],
        'bootstrap' : [True, False]
    }, 
    cv=5, 
    return_train_score=False, 
    n_iter=2
)
rs.fit(reports, reports['No. of deaths'])
pd.DataFrame(rs.cv_results_)[['param_n_estimators','param_max_features','param_max_depth','param_min_samples_split','param_min_samples_leaf','param_bootstrap','mean_test_score']]



rs.fit(X_train,y_train)

yp = rs.predict(X_test)

r2_score(y_test, yp)

rs.best_params_

rf = RandomForestRegressor(n_estimators =1200,min_samples_split = 2,min_samples_leaf = 1,max_features = 'auto',max_depth = 40,bootstrap = True)

rf.fit(X_train,y_train)

pr = rf.predict(X_test)

pred = rf.predict(X_test)

r2_score(y_test, pr)

from sklearn.metrics import mean_squared_error

from sklearn.metrics import mean_absolute_error

from sklearn.metrics import mean_squared_error
import math
print(mean_squared_error(y_test, pred))
print(math.sqrt(mean_squared_error(y_test, pred)))

import matplotlib.pyplot as plt

from xgboost import XGBRegressor
from xgboost import XGBRFRegressor
from sklearn.model_selection import GridSearchCV

from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error

def hyperParameterTuning(X_train, y_train):
    param_tuning = {
        'learning_rate': [0.05, 0.10, 0.15, 0.20, 0.25, 0.30],
        'max_depth': [3, 4, 5, 6, 8, 10, 12, 15],
        'min_child_weight': [1, 3, 5,6,8],
        'subsample': [0.5, 0.7],
        'colsample_bytree': [0.3,0.2,0.5, 0.7],
         "gamma"            : [ 0.0, 0.1, 0.2 , 0.3, 0.4 ],
        'n_estimators' : [50,68,85,100, 200, 500]}
        
        

    xgb_model = XGBRegressor()

    gsearch = GridSearchCV(estimator = xgb_model,
                           param_grid = param_tuning,                        
                           #scoring = 'neg_mean_absolute_error', #MAE
                           #scoring = 'neg_mean_squared_error',  #MSE
                           cv = 5,
                           n_jobs = -1,
                           verbose = 3)

    gsearch.fit(X_train,y_train)

    return gsearch.best_params_

hyperParameterTuning(X_train, y_train)

# Commented out IPython magic to ensure Python compatibility.
xgb_model = XGBRegressor(
        objective = 'reg:squarederror',
        colsample_bytree = 0.5,
        learning_rate = 0.25,
        gamma = 0.4,
        max_depth = 10,
        min_child_weight = 1,
        n_estimators = 50,
        subsample = 0.7)

# %time xgb_model.fit(X_train, y_train, early_stopping_rounds=5, eval_set=[(X_test, y_test)], verbose=False)

y_pred_xgb = xgb_model.predict(X_test)

mae_xgb = mean_absolute_error(y_test, y_pred_xgb)

print("MAE: ", mae_xgb)

r2_score(y_test, y_pred_xgb)

import statsmodels.api as sm

cols = list(X.columns)
pmax = 1
while (len(cols)>0):
    p= []
    X_1 = X[cols]
    X_1 = sm.add_constant(X_1)
    model = sm.OLS(y,X_1).fit()
    p = pd.Series(model.pvalues.values[1:],index = cols)      
    pmax = max(p)
    feature_with_p_max = p.idxmax()
    if(pmax>0.1):
        cols.remove(feature_with_p_max)
    else:
        break
selected_features_BE = cols
print(selected_features_BE)





